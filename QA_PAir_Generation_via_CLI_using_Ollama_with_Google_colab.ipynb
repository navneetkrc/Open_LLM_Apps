{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/Open_LLM_Apps/blob/main/QA_PAir_Generation_via_CLI_using_Ollama_with_Google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApKoENkyzqay"
      },
      "outputs": [],
      "source": [
        "# Cell 1 - Install Required Libraries:\n",
        "!pip install -q streamlit pymupdf4llm==0.0.17 pandas openpyxl pyngrok requests langchain-community PyPDF2  # Added 'requests' for Ollama API calls\n",
        "\n",
        "# Install Ollama (using a convenient installation script)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install and setup ngrok (as in your original code)\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - Import Libraries and Set Up Environment:\n",
        "import streamlit as st\n",
        "# import fitz  # PyMuPDF4LLM\n",
        "import pymupdf4llm\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import requests  # For making requests to the Ollama API\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import userdata\n",
        "# Remove Groq API key setup\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get('groq_colab_key') #krch\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get('groq_604779')\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "# Get ngrok auth token from environment\n",
        "ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "!ngrok authtoken {ngrok_token}\n",
        "\n",
        "# Start Ollama in the background (replace 'model_one' and 'model_two' with your actual model names)\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "!ollama pull gemma2  # Example - replace with your actual model names\n",
        "!ollama pull llama3.2 # Example - replace with your actual model names\n",
        "time.sleep(10) # Give Ollama time to start"
      ],
      "metadata": {
        "id": "8hrNMdGgCUsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Configure page settings\n",
        "st.set_page_config(page_title=\"Text Processor\", page_icon=\"📝\")\n",
        "\n",
        "# Define constants\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/\"\n",
        "OLLAMA_MODEL_NAME = \"gemma2\"\n",
        "\n",
        "def generate_questions(content):\n",
        "    \"\"\"\n",
        "    Generates questions from the given content using Ollama model\n",
        "    Args:\n",
        "        content (str): Text content to generate questions from\n",
        "    Returns:\n",
        "        list: List of generated questions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Creating Ollama client\n",
        "        ollama = Ollama(base_url=OLLAMA_BASE_URL, model=OLLAMA_MODEL_NAME)\n",
        "\n",
        "        # Improved prompt design for question generation\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following text, generate exactly 3 meaningful questions that test understanding\n",
        "        of the key concepts. The questions should be diverse and cover different aspects of the content.\n",
        "\n",
        "        Text:\n",
        "        {content.strip()}\n",
        "\n",
        "        Please provide exactly 3 questions, each on a new line.\n",
        "        \"\"\"\n",
        "\n",
        "        # Making the API call\n",
        "        response = ollama.invoke(prompt)\n",
        "\n",
        "        # Split the response into individual questions\n",
        "        questions = [q.strip() for q in response.strip().split('\\n') if q.strip()]\n",
        "\n",
        "        # Ensure we have exactly 3 questions\n",
        "        return questions[:3]\n",
        "    except Exception as e:\n",
        "        return [f\"Error: An error occurred during question generation: {e}\"]\n",
        "\n",
        "def process_text_chunks(content, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Splits text into chunks and generates questions for each chunk\n",
        "    Args:\n",
        "        content (str): Text content to process\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "    Returns:\n",
        "        dict: Dictionary mapping chunk numbers to lists of questions\n",
        "    \"\"\"\n",
        "    # Split text into chunks (simple splitting by characters)\n",
        "    chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "\n",
        "    # Generate questions for each chunk\n",
        "    questions_by_chunk = {}\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        questions = generate_questions(chunk)\n",
        "        questions_by_chunk[f\"Chunk {i}\"] = questions\n",
        "\n",
        "    return questions_by_chunk\n",
        "\n",
        "def main():\n",
        "    # Add title and description\n",
        "    st.title(\"📝 Generating Questions from Text Chunks\")\n",
        "    st.markdown(\"Upload a text file or enter text directly to generate questions from the content.\")\n",
        "\n",
        "    # Create tabs for different input methods\n",
        "    tab1, tab2 = st.tabs([\"Enter Text\", \"Upload File\"])\n",
        "\n",
        "    with tab1:\n",
        "        # Text input\n",
        "        text_input = st.text_area(\"Enter your text here:\", height=200)\n",
        "        chunk_size = st.number_input(\"Chunk size (characters):\", min_value=100, value=500, step=100)\n",
        "\n",
        "        if st.button(\"Generate Questions\", key=\"process_text\"):\n",
        "            if text_input:\n",
        "                with st.spinner(\"Generating questions...\"):\n",
        "                    questions_by_chunk = process_text_chunks(text_input, chunk_size)\n",
        "\n",
        "                    # Display questions for each chunk\n",
        "                    for chunk_num, questions in questions_by_chunk.items():\n",
        "                        st.subheader(f\"Questions for {chunk_num}:\")\n",
        "                        for i, question in enumerate(questions, 1):\n",
        "                            st.write(f\"{i}. {question}\")\n",
        "                        st.markdown(\"---\")\n",
        "            else:\n",
        "                st.error(\"Please enter some text to process.\")\n",
        "\n",
        "    with tab2:\n",
        "        # File upload\n",
        "        uploaded_file = st.file_uploader(\"Choose a text file\", type=['txt'])\n",
        "        if uploaded_file is not None:\n",
        "            content = uploaded_file.read().decode()\n",
        "            st.text_area(\"File content:\", content, height=200)\n",
        "            chunk_size = st.number_input(\"Chunk size (characters):\", min_value=100, value=500, step=100, key=\"file_chunk_size\")\n",
        "\n",
        "            if st.button(\"Generate Questions\", key=\"process_file\"):\n",
        "                with st.spinner(\"Generating questions...\"):\n",
        "                    questions_by_chunk = process_text_chunks(content, chunk_size)\n",
        "\n",
        "                    # Display questions for each chunk\n",
        "                    for chunk_num, questions in questions_by_chunk.items():\n",
        "                        st.subheader(f\"Questions for {chunk_num}:\")\n",
        "                        for i, question in enumerate(questions, 1):\n",
        "                            st.write(f\"{i}. {question}\")\n",
        "                        st.markdown(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfUOoMg_Bj7U",
        "outputId": "c11b78ff-e9f5-4e55-e8fe-52583067d5b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill any existing Streamlit processes\n",
        "!kill -9 $(pgrep streamlit) 2>/dev/null\n",
        "!nohup ollama serve > ollama.log 2>&1 & #uncomment from 2nd run as ollama closes\n",
        "time.sleep(5)\n",
        "\n",
        "# Start Streamlit\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "time.sleep(5)\n",
        "\n",
        "# Create ngrok tunnel with correct configuration\n",
        "ngrok_tunnel = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "print(f\"Streamlit app URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Keep the tunnel open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Closing ngrok tunnel...\")\n",
        "    ngrok.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8-jgosrBs0S",
        "outputId": "ad03bb73-3a37-42cf-c63a-fb9720a75ac1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app URL: https://97c5-34-125-243-139.ngrok-free.app\n",
            "Closing ngrok tunnel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_with_pdf_text.py\n",
        "import streamlit as st\n",
        "from langchain_community.llms import Ollama\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pymupdf4llm\n",
        "import io\n",
        "\n",
        "# Configure page settings\n",
        "st.set_page_config(page_title=\"Generate QA Pairs from text Chunks\", page_icon=\"📝\")\n",
        "\n",
        "# Define constants\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/\"\n",
        "OLLAMA_MODEL_NAME = \"gemma2\"\n",
        "\n",
        "def generate_qa_pairs_new(content):\n",
        "    \"\"\"\n",
        "    Generates question-answer pairs from the given content using Ollama model\n",
        "    Args:\n",
        "        content (str): Text content to generate QA pairs from\n",
        "    Returns:\n",
        "        list: List of dictionaries containing questions, answers, and context\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Creating Ollama client\n",
        "        ollama = Ollama(base_url=OLLAMA_BASE_URL, model=OLLAMA_MODEL_NAME)\n",
        "\n",
        "        # Modified prompt design with stricter formatting requirements\n",
        "        prompt = f\"\"\"\n",
        "        Generate 3 question-answer pairs based on the following text. Make sure each question is specific and answerable from the given context.\n",
        "        Use this exact format, starting each pair with \"Q:\" and \"A:\" on new lines:\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Text to use:\n",
        "        {content.strip()}\n",
        "        \"\"\"\n",
        "\n",
        "        # Making the API call\n",
        "        response = ollama.invoke(prompt)\n",
        "\n",
        "        # Parse the response into QA pairs using a more robust method\n",
        "        qa_pairs = []\n",
        "        lines = response.strip().split('\\n')\n",
        "        current_question = None\n",
        "        current_answer = None\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:  # Skip empty lines\n",
        "                continue\n",
        "\n",
        "            if line.startswith('Q:'):\n",
        "                # If we have a complete pair, add it\n",
        "                if current_question and current_answer:\n",
        "                    qa_pairs.append({\n",
        "                        'question': current_question,\n",
        "                        'answer': current_answer,\n",
        "                        'context': content.strip()\n",
        "                    })\n",
        "                current_question = line[2:].strip()\n",
        "                current_answer = None\n",
        "            elif line.startswith('A:'):\n",
        "                current_answer = line[2:].strip()\n",
        "\n",
        "        # Add the last pair if complete\n",
        "        if current_question and current_answer:\n",
        "            qa_pairs.append({\n",
        "                'question': current_question,\n",
        "                'answer': current_answer,\n",
        "                'context': content.strip()\n",
        "            })\n",
        "\n",
        "        # If we didn't get any valid pairs, generate a fallback pair\n",
        "        if not qa_pairs:\n",
        "            # Generate at least one fallback question-answer pair\n",
        "            fallback_prompt = f\"\"\"\n",
        "            Generate one clear question and its answer from this text:\n",
        "            {content.strip()}\n",
        "            Format as:\n",
        "            Q: [question]\n",
        "            A: [answer]\n",
        "            \"\"\"\n",
        "            fallback_response = ollama.invoke(fallback_prompt)\n",
        "            lines = fallback_response.strip().split('\\n')\n",
        "            for line in lines:\n",
        "                if line.startswith('Q:'):\n",
        "                    current_question = line[2:].strip()\n",
        "                elif line.startswith('A:'):\n",
        "                    current_answer = line[2:].strip()\n",
        "\n",
        "            if current_question and current_answer:\n",
        "                qa_pairs.append({\n",
        "                    'question': current_question,\n",
        "                    'answer': current_answer,\n",
        "                    'context': content.strip()\n",
        "                })\n",
        "\n",
        "        # Validate and clean up the QA pairs\n",
        "        validated_pairs = []\n",
        "        for pair in qa_pairs:\n",
        "            # Ensure both question and answer exist and are not empty\n",
        "            if pair['question'] and pair['answer'] and \\\n",
        "               len(pair['question'].strip()) > 0 and \\\n",
        "               len(pair['answer'].strip()) > 0:\n",
        "                validated_pairs.append(pair)\n",
        "\n",
        "        return validated_pairs if validated_pairs else [{\n",
        "            'question': 'What is the main topic of this text?',\n",
        "            'answer': 'The text discusses ' + content[:50] + '...',\n",
        "            'context': content.strip()\n",
        "        }]\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in QA generation: {str(e)}\")\n",
        "        return [{\n",
        "            'question': 'What is the main topic of this text?',\n",
        "            'answer': 'Error occurred during processing. Please try again.',\n",
        "            'context': content.strip()\n",
        "        }]\n",
        "\n",
        "# [Rest of the code remains the same]\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file\n",
        "    Args:\n",
        "        pdf_file: Uploaded PDF file object\n",
        "    Returns:\n",
        "        str: Extracted text content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a PDF reader object\n",
        "        pdf_reader = pymupdf4llm.to_markdown(io.BytesIO(pdf_file.read()))\n",
        "\n",
        "        # Extract text from all pages\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error extracting text from PDF: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def process_text_chunks(content, filename, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Splits text into chunks and generates QA pairs for each chunk\n",
        "    Args:\n",
        "        content (str): Text content to process\n",
        "        filename (str): Name of the input file\n",
        "        chunk_size (int): Size of each chunk in characters\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing all QA pairs with metadata\n",
        "    \"\"\"\n",
        "    # Split text into chunks\n",
        "    chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "\n",
        "    # Generate QA pairs for each chunk and collect data for DataFrame\n",
        "    data = []\n",
        "    for chunk_num, chunk in enumerate(chunks, 1):\n",
        "        qa_pairs = generate_qa_pairs_new(chunk)\n",
        "        for qa_pair in qa_pairs:\n",
        "            data.append({\n",
        "                'pdf_name': filename,\n",
        "                'chunk_number': chunk_num,\n",
        "                'question': qa_pair.get('question', ''),\n",
        "                'answer': qa_pair.get('answer', ''),\n",
        "                'context': qa_pair.get('context', '')\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def save_to_csv(df):\n",
        "    \"\"\"\n",
        "    Saves the DataFrame to a CSV file with timestamp\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame to save\n",
        "    Returns:\n",
        "        str: Path to the saved file\n",
        "    \"\"\"\n",
        "    # Create 'outputs' directory if it doesn't exist\n",
        "    if not os.path.exists('outputs'):\n",
        "        os.makedirs('outputs')\n",
        "\n",
        "    # Generate filename with timestamp\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f'outputs/qa_pairs_{timestamp}.csv'\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    df.to_csv(filename, index=False)\n",
        "    return filename\n",
        "\n",
        "def main():\n",
        "    # Add title and description\n",
        "    st.title(\"📝 Text Processor with Q&A Generation\")\n",
        "    st.markdown(\"Upload a text file or enter text directly to generate question-answer pairs and save to CSV.\")\n",
        "\n",
        "    # Create tabs for different input methods\n",
        "    tab1, tab2 = st.tabs([\"Enter Text\", \"Upload File\"])\n",
        "\n",
        "    with tab1:\n",
        "        # Text input\n",
        "        text_input = st.text_area(\"Enter your text here:\", height=200)\n",
        "        chunk_size = st.number_input(\"Chunk size (characters):\", min_value=100, value=500, step=100)\n",
        "\n",
        "        if st.button(\"Generate Q&A Pairs\", key=\"process_text\"):\n",
        "            if text_input:\n",
        "                with st.spinner(\"Generating Q&A pairs...\"):\n",
        "                    df = process_text_chunks(text_input, \"manual_input.txt\", chunk_size)\n",
        "\n",
        "                    # Display the Q&A pairs\n",
        "                    st.subheader(\"Generated Q&A Pairs:\")\n",
        "                    for chunk_num in df['chunk_number'].unique():\n",
        "                        chunk_data = df[df['chunk_number'] == chunk_num]\n",
        "                        st.write(f\"### Chunk {chunk_num}\")\n",
        "                        for _, row in chunk_data.iterrows():\n",
        "                            st.write(f\"**Q:** {row['question']}\")\n",
        "                            st.write(f\"**A:** {row['answer']}\")\n",
        "                            st.write(\"**Context:**\")\n",
        "                            st.write(row['context'])\n",
        "                            st.markdown(\"---\")\n",
        "\n",
        "                    # Save to CSV\n",
        "                    csv_path = save_to_csv(df)\n",
        "                    st.success(f\"Q&A pairs saved to: {csv_path}\")\n",
        "\n",
        "                    # Add download button\n",
        "                    with open(csv_path, 'rb') as file:\n",
        "                        st.download_button(\n",
        "                            label=\"Download CSV\",\n",
        "                            data=file,\n",
        "                            file_name=os.path.basename(csv_path),\n",
        "                            mime='text/csv'\n",
        "                        )\n",
        "            else:\n",
        "                st.error(\"Please enter some text to process.\")\n",
        "\n",
        "    with tab2:\n",
        "        # File upload with support for PDF and TXT\n",
        "        uploaded_file = st.file_uploader(\"Choose a file\", type=['txt', 'pdf'])\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            # Extract text based on file type\n",
        "            file_extension = uploaded_file.name.split('.')[-1].lower()\n",
        "\n",
        "            if file_extension == 'pdf':\n",
        "                content = extract_text_from_pdf(uploaded_file)\n",
        "                if not content:\n",
        "                    st.error(\"Unable to extract text from PDF. Please check if the PDF is text-based and not scanned.\")\n",
        "                    st.stop()\n",
        "            else:  # txt file\n",
        "                content = uploaded_file.read().decode('utf-8', errors='ignore')\n",
        "\n",
        "            # Display extracted content\n",
        "            st.text_area(\"Extracted content:\", content, height=200)\n",
        "\n",
        "            chunk_size = st.number_input(\"Chunk size (characters):\", min_value=100, value=500, step=100, key=\"file_chunk_size\")\n",
        "\n",
        "            if st.button(\"Generate Q&A Pairs\", key=\"process_file\"):\n",
        "                with st.spinner(\"Generating Q&A pairs...\"):\n",
        "                    df = process_text_chunks(content, uploaded_file.name, chunk_size)\n",
        "\n",
        "                    # Display the Q&A pairs\n",
        "                    st.subheader(\"Generated Q&A Pairs:\")\n",
        "                    for chunk_num in df['chunk_number'].unique():\n",
        "                        chunk_data = df[df['chunk_number'] == chunk_num]\n",
        "                        st.write(f\"### Chunk {chunk_num}\")\n",
        "                        for _, row in chunk_data.iterrows():\n",
        "                            st.write(f\"**Q:** {row['question']}\")\n",
        "                            st.write(f\"**A:** {row['answer']}\")\n",
        "                            st.write(\"**Context:**\")\n",
        "                            st.write(row['context'])\n",
        "                            st.markdown(\"---\")\n",
        "\n",
        "                    # Save to CSV\n",
        "                    csv_path = save_to_csv(df)\n",
        "                    st.success(f\"Q&A pairs saved to: {csv_path}\")\n",
        "\n",
        "                    # Add download button\n",
        "                    with open(csv_path, 'rb') as file:\n",
        "                        st.download_button(\n",
        "                            label=\"Download CSV\",\n",
        "                            data=file,\n",
        "                            file_name=os.path.basename(csv_path),\n",
        "                            mime='text/csv'\n",
        "                        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmEyQT-_dv6T",
        "outputId": "9419d164-d319-4736-83d3-25ebc1787796"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_with_pdf_text.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill any existing Streamlit processes\n",
        "!kill -9 $(pgrep streamlit) 2>/dev/null\n",
        "time.sleep(10)\n",
        "\n",
        "# Start Streamlit\n",
        "!streamlit run app_with_pdf_text.py &>/content/logs.txt &\n",
        "!nohup ollama serve > ollama.log 2>&1 & #Uncomment this from 2nd run onwards as ollama serve is closed/interrupted\n",
        "time.sleep(10)\n",
        "# Create ngrok tunnel with correct configuration\n",
        "ngrok_tunnel = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "print(f\"Streamlit app URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Keep the tunnel open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Closing ngrok tunnel...\")\n",
        "    ngrok.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMiPiJ8XfHfn",
        "outputId": "6c28b7e4-1165-4caa-b24a-4b7b288930ae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app URL: https://c0bd-34-125-243-139.ngrok-free.app\n",
            "Closing ngrok tunnel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve > ollama.log 2>&1 & #Uncomment this from 2nd run onwards as ollama serve is closed/interrupted\n",
        "\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8rsOt7EBj9_",
        "outputId": "97b03afe-a657-480a-ada2-fdbe251c3b94"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME               ID              SIZE      MODIFIED      \n",
            "gemma2:latest      ff02c3702f32    5.4 GB    5 minutes ago    \n",
            "llama3.2:latest    a80c4f17acd5    2.0 GB    5 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Yb_YPLIBkAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZgJ_P1XBkCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CLI Version"
      ],
      "metadata": {
        "id": "dofcMofovZQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_generator_cli.py\n",
        "import argparse\n",
        "from langchain_community.llms import Ollama\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import PyPDF2\n",
        "import io\n",
        "import sys\n",
        "\n",
        "# Define constants\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/\"\n",
        "OLLAMA_MODEL_NAME = \"gemma2\"\n",
        "\n",
        "def generate_qa_pairs(content):\n",
        "    \"\"\"\n",
        "    Generates question-answer pairs from the given content using Ollama model\n",
        "    Args:\n",
        "        content (str): Text content to generate QA pairs from\n",
        "    Returns:\n",
        "        list: List of dictionaries containing questions, answers, and context\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Creating Ollama client\n",
        "        ollama = Ollama(base_url=OLLAMA_BASE_URL, model=OLLAMA_MODEL_NAME)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Generate 3 question-answer pairs based on the following text. Make sure each question is specific and answerable from the given context.\n",
        "        Use this exact format, starting each pair with \"Q:\" and \"A:\" on new lines:\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Text to use:\n",
        "        {content.strip()}\n",
        "        \"\"\"\n",
        "\n",
        "        response = ollama.invoke(prompt)\n",
        "\n",
        "        qa_pairs = []\n",
        "        lines = response.strip().split('\\n')\n",
        "        current_question = None\n",
        "        current_answer = None\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            if line.startswith('Q:'):\n",
        "                if current_question and current_answer:\n",
        "                    qa_pairs.append({\n",
        "                        'question': current_question,\n",
        "                        'answer': current_answer,\n",
        "                        'context': content.strip()\n",
        "                    })\n",
        "                current_question = line[2:].strip()\n",
        "                current_answer = None\n",
        "            elif line.startswith('A:'):\n",
        "                current_answer = line[2:].strip()\n",
        "\n",
        "        if current_question and current_answer:\n",
        "            qa_pairs.append({\n",
        "                'question': current_question,\n",
        "                'answer': current_answer,\n",
        "                'context': content.strip()\n",
        "            })\n",
        "\n",
        "        return qa_pairs if qa_pairs else [{\n",
        "            'question': 'What is the main topic of this text?',\n",
        "            'answer': 'The text discusses ' + content[:50] + '...',\n",
        "            'context': content.strip()\n",
        "        }]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in QA generation: {str(e)}\", file=sys.stderr)\n",
        "        return [{\n",
        "            'question': 'What is the main topic of this text?',\n",
        "            'answer': 'Error occurred during processing. Please try again.',\n",
        "            'context': content.strip()\n",
        "        }]\n",
        "\n",
        "def process_text_chunks(content, chunk_size=500, overlap=100):\n",
        "    \"\"\"\n",
        "    Splits text into chunks with overlap and generates QA pairs for each chunk\n",
        "    Args:\n",
        "        content (str): Text content to process\n",
        "        chunk_size (int): Size of text chunks in characters\n",
        "        overlap (int): Number of characters to overlap between chunks\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing all QA pairs with metadata\n",
        "    \"\"\"\n",
        "    # Split content into words to ensure we don't cut words in middle\n",
        "    words = content.split()\n",
        "\n",
        "    # Initialize variables for chunking\n",
        "    chunks = []\n",
        "    chunk_words = []\n",
        "    current_size = 0\n",
        "    last_chunk_end = 0\n",
        "\n",
        "    # Create chunks with overlap\n",
        "    for i, word in enumerate(words):\n",
        "        chunk_words.append(word)\n",
        "        current_size += len(word) + 1  # +1 for space\n",
        "\n",
        "        # Check if we've reached chunk size\n",
        "        if current_size >= chunk_size:\n",
        "            # Join words to form chunk\n",
        "            chunk = ' '.join(chunk_words)\n",
        "            chunks.append(chunk)\n",
        "\n",
        "            # Calculate how many words to keep for overlap\n",
        "            # First, find the last 'overlap' characters worth of words\n",
        "            overlap_start = max(0, len(chunk) - overlap)\n",
        "            overlap_text = chunk[overlap_start:]\n",
        "            overlap_words = overlap_text.split()\n",
        "\n",
        "            # Reset for next chunk, starting with overlap words\n",
        "            chunk_words = overlap_words\n",
        "            current_size = sum(len(word) + 1 for word in overlap_words)\n",
        "            last_chunk_end = i - len(overlap_words)\n",
        "\n",
        "    # Add the remaining text as the last chunk if there's any\n",
        "    if chunk_words:\n",
        "        chunks.append(' '.join(chunk_words))\n",
        "\n",
        "    # Process chunks and generate QA pairs\n",
        "    data = []\n",
        "    total_chunks = len(chunks)\n",
        "    print(f\"Processing {total_chunks} chunks...\")\n",
        "\n",
        "    for chunk_num, chunk in enumerate(chunks, 1):\n",
        "        print(f\"Processing chunk {chunk_num}/{total_chunks}\")\n",
        "        qa_pairs = generate_qa_pairs(chunk)\n",
        "        for qa_pair in qa_pairs:\n",
        "            data.append({\n",
        "                'chunk_number': chunk_num,\n",
        "                'question': qa_pair.get('question', ''),\n",
        "                'answer': qa_pair.get('answer', ''),\n",
        "                'context': qa_pair.get('context', ''),\n",
        "                'chunk_start': chunk[:50] + '...',  # First 50 chars of chunk for reference\n",
        "                'chunk_end': '...' + chunk[-50:]    # Last 50 chars of chunk for reference\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def save_to_csv(df, output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Saves the DataFrame to a CSV file with timestamp in the specified directory\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame to save\n",
        "        output_dir (str): Directory to save the CSV file\n",
        "    Returns:\n",
        "        str: Path to the saved file\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = os.path.join(output_dir, f'qa_pairs_{timestamp}.csv')\n",
        "\n",
        "    df.to_csv(filename, index=False)\n",
        "    return filename\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "    Returns:\n",
        "        str: Extracted text content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "            return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {str(e)}\", file=sys.stderr)\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Generate Q&A pairs from text content')\n",
        "    parser.add_argument('input', help='Input file path (PDF or TXT) or text string')\n",
        "    parser.add_argument('--chunk-size', type=int, default=500, help='Size of text chunks (default: 500)')\n",
        "    parser.add_argument('--output-dir', default=\".\", help='Directory to save the output CSV file (default: current directory)')\n",
        "    parser.add_argument('--is-file', action='store_true', help='Treat input as a file path')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    try:\n",
        "        # Process input\n",
        "        if args.is_file:\n",
        "            if not os.path.exists(args.input):\n",
        "                print(f\"Error: File not found: {args.input}\", file=sys.stderr)\n",
        "                sys.exit(1)\n",
        "\n",
        "            file_extension = os.path.splitext(args.input)[1].lower()\n",
        "            if file_extension == '.pdf':\n",
        "                content = extract_text_from_pdf(args.input)\n",
        "                if not content:\n",
        "                    print(\"Error: Could not extract text from PDF\", file=sys.stderr)\n",
        "                    sys.exit(1)\n",
        "            else:  # txt file\n",
        "                with open(args.input, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "\n",
        "            filename = os.path.basename(args.input)\n",
        "        else:\n",
        "            content = args.input\n",
        "            filename = \"input_text\"\n",
        "\n",
        "        # Process the content\n",
        "        df = process_text_chunks(content, filename, args.chunk_size)\n",
        "\n",
        "        # Save to CSV\n",
        "        output_path = save_to_csv(df, args.output_dir)\n",
        "        print(f\"\\nQ&A pairs saved to: {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "EZyNGv6aJIDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a200e0a-8687-4d94-ef63-e33584670ac1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing qa_generator_cli.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python qa_generator_cli.py\n",
        "\"\"\" Title: Fixing Heating Issues in Samsung Microwave Ovens\n",
        "Samsung microwaves are designed for efficient cooking and reheating. If the microwave is not heating properly, follow these steps:\n",
        "Inspect Power Supply:\n",
        "Ensure the microwave is properly plugged into a functional power outlet.\n",
        "Check for visible damage to the power cord.\n",
        "Check Door Seal:\n",
        "Ensure the microwave door is fully closed and the seals are clean and intact.\n",
        "Test Cooking Mode:\n",
        "Place a cup of water inside and run the microwave on high power for 1-2 minutes. If the water doesn’t heat, the issue may be with the magnetron or other internal components.\n",
        "Reset Microwave:\n",
        "Unplug the unit for 30 seconds, then plug it back in and restart.\n",
        "Contact Samsung Service:\n",
        "If none of these steps work, contact Samsung Support with the model and serial number of your microwave for further assistance.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "SprU5Hy2v1lF",
        "outputId": "50a944cc-765a-482e-a386-6525b589c531"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: qa_generator_cli.py [-h] [--chunk-size CHUNK_SIZE] [--output-dir OUTPUT_DIR] [--is-file]\n",
            "                           input\n",
            "qa_generator_cli.py: error: the following arguments are required: input\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Title: Fixing Heating Issues in Samsung Microwave Ovens\\nSamsung microwaves are designed for efficient cooking and reheating. If the microwave is not heating properly, follow these steps:\\nInspect Power Supply:\\nEnsure the microwave is properly plugged into a functional power outlet.\\nCheck for visible damage to the power cord.\\nCheck Door Seal:\\nEnsure the microwave door is fully closed and the seals are clean and intact.\\nTest Cooking Mode:\\nPlace a cup of water inside and run the microwave on high power for 1-2 minutes. If the water doesn’t heat, the issue may be with the magnetron or other internal components.\\nReset Microwave:\\nUnplug the unit for 30 seconds, then plug it back in and restart.\\nContact Samsung Service:\\nIf none of these steps work, contact Samsung Support with the model and serial number of your microwave for further assistance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_generator.py\n",
        "from langchain_community.llms import Ollama\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Define constants\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/\"\n",
        "OLLAMA_MODEL_NAME = \"gemma2\"\n",
        "\n",
        "def generate_qa_pairs(content):\n",
        "    \"\"\"\n",
        "    Generates question-answer pairs from the given content using Ollama model\n",
        "    Args:\n",
        "        content (str): Text content to generate QA pairs from\n",
        "    Returns:\n",
        "        list: List of dictionaries containing questions, answers, and context\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Creating Ollama client\n",
        "        ollama = Ollama(base_url=OLLAMA_BASE_URL, model=OLLAMA_MODEL_NAME)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Generate 3 question-answer pairs based on the following text. Make sure each question is specific and answerable from the given context.\n",
        "        Use this exact format, starting each pair with \"Q:\" and \"A:\" on new lines:\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Q: [Write a specific question about the content]\n",
        "        A: [Write the corresponding answer]\n",
        "\n",
        "        Text to use:\n",
        "        {content.strip()}\n",
        "        \"\"\"\n",
        "\n",
        "        response = ollama.invoke(prompt)\n",
        "\n",
        "        qa_pairs = []\n",
        "        lines = response.strip().split('\\n')\n",
        "        current_question = None\n",
        "        current_answer = None\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            if line.startswith('Q:'):\n",
        "                if current_question and current_answer:\n",
        "                    qa_pairs.append({\n",
        "                        'question': current_question,\n",
        "                        'answer': current_answer,\n",
        "                        'context': content.strip()\n",
        "                    })\n",
        "                current_question = line[2:].strip()\n",
        "                current_answer = None\n",
        "            elif line.startswith('A:'):\n",
        "                current_answer = line[2:].strip()\n",
        "\n",
        "        if current_question and current_answer:\n",
        "            qa_pairs.append({\n",
        "                'question': current_question,\n",
        "                'answer': current_answer,\n",
        "                'context': content.strip()\n",
        "            })\n",
        "\n",
        "        return qa_pairs if qa_pairs else [{\n",
        "            'question': 'What is the main topic of this text?',\n",
        "            'answer': 'The text discusses ' + content[:50] + '...',\n",
        "            'context': content.strip()\n",
        "        }]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in QA generation: {str(e)}\")\n",
        "        return [{\n",
        "            'question': 'What is the main topic of this text?',\n",
        "            'answer': 'Error occurred during processing. Please try again.',\n",
        "            'context': content.strip()\n",
        "        }]\n",
        "\n",
        "def process_text_chunks(content, chunk_size=500, overlap=100):\n",
        "    \"\"\"\n",
        "    Splits text into chunks with overlap and generates QA pairs for each chunk\n",
        "    Args:\n",
        "        content (str): Text content to process\n",
        "        chunk_size (int): Size of text chunks in characters\n",
        "        overlap (int): Number of characters to overlap between chunks\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing all QA pairs with metadata\n",
        "    \"\"\"\n",
        "    # Split content into words to ensure we don't cut words in middle\n",
        "    words = content.split()\n",
        "\n",
        "    # Initialize variables for chunking\n",
        "    chunks = []\n",
        "    chunk_words = []\n",
        "    current_size = 0\n",
        "    last_chunk_end = 0\n",
        "\n",
        "    # Create chunks with overlap\n",
        "    for i, word in enumerate(words):\n",
        "        chunk_words.append(word)\n",
        "        current_size += len(word) + 1  # +1 for space\n",
        "\n",
        "        # Check if we've reached chunk size\n",
        "        if current_size >= chunk_size:\n",
        "            # Join words to form chunk\n",
        "            chunk = ' '.join(chunk_words)\n",
        "            chunks.append(chunk)\n",
        "\n",
        "            # Calculate how many words to keep for overlap\n",
        "            # First, find the last 'overlap' characters worth of words\n",
        "            overlap_start = max(0, len(chunk) - overlap)\n",
        "            overlap_text = chunk[overlap_start:]\n",
        "            overlap_words = overlap_text.split()\n",
        "\n",
        "            # Reset for next chunk, starting with overlap words\n",
        "            chunk_words = overlap_words\n",
        "            current_size = sum(len(word) + 1 for word in overlap_words)\n",
        "            last_chunk_end = i - len(overlap_words)\n",
        "\n",
        "    # Add the remaining text as the last chunk if there's any\n",
        "    if chunk_words:\n",
        "        chunks.append(' '.join(chunk_words))\n",
        "\n",
        "    # Process chunks and generate QA pairs\n",
        "    data = []\n",
        "    total_chunks = len(chunks)\n",
        "    print(f\"Processing {total_chunks} chunks...\")\n",
        "\n",
        "    for chunk_num, chunk in enumerate(chunks, 1):\n",
        "        print(f\"Processing chunk {chunk_num}/{total_chunks}\")\n",
        "        qa_pairs = generate_qa_pairs(chunk)\n",
        "        for qa_pair in qa_pairs:\n",
        "            data.append({\n",
        "                'chunk_number': chunk_num,\n",
        "                'question': qa_pair.get('question', ''),\n",
        "                'answer': qa_pair.get('answer', ''),\n",
        "                'context': qa_pair.get('context', ''),\n",
        "                'chunk_start': chunk[:50] + '...',  # First 50 chars of chunk for reference\n",
        "                'chunk_end': '...' + chunk[-50:]    # Last 50 chars of chunk for reference\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def generate_qa(text, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Main function to generate QA pairs from text\n",
        "    Args:\n",
        "        text (str): Input text content\n",
        "        chunk_size (int): Size of text chunks\n",
        "    Returns:\n",
        "        str: Path to the saved CSV file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Process the content\n",
        "        df = process_text_chunks(text, chunk_size)\n",
        "\n",
        "        # Save to CSV\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_path = f'qa_pairs_{timestamp}.csv'\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"\\nQ&A pairs saved to: {output_path}\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage in Colab:\n",
        "if __name__ == \"__main__\":\n",
        "    # This code won't run when imported as a module\n",
        "    text = \"\"\"Your sample text here. This will be processed into Q&A pairs.\"\"\"\n",
        "    output_file = generate_qa(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moIvyTKLv1n4",
        "outputId": "9dd8bc9c-2d3a-48b8-b0cd-36487f196544"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting qa_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qa_generator import generate_qa\n",
        "\n",
        "# Your text content\n",
        "text = \"\"\"Title: Fixing Heating Issues in Samsung Microwave Ovens\n",
        "Samsung microwaves are designed for efficient cooking and reheating. If the microwave is not heating properly, follow these steps:\n",
        "Inspect Power Supply:\n",
        "Ensure the microwave is properly plugged into a functional power outlet.\n",
        "Check for visible damage to the power cord.\n",
        "Check Door Seal:\n",
        "Ensure the microwave door is fully closed and the seals are clean and intact.\n",
        "Test Cooking Mode:\n",
        "Place a cup of water inside and run the microwave on high power for 1-2 minutes. If the water doesn’t heat, the issue may be with the magnetron or other internal components.\n",
        "Reset Microwave:\n",
        "Unplug the unit for 30 seconds, then plug it back in and restart.\n",
        "Contact Samsung Service:\n",
        "If none of these steps work, contact Samsung Support with the model and serial number of your microwave for further assistance.\"\"\"\n",
        "\n",
        "# Generate Q&A pairs and save to CSV\n",
        "output_file = generate_qa(text)  # Uses default chunk size of 500\n",
        "## OR\n",
        "# output_file = generate_qa(text, chunk_size=1000)  # Specify custom chunk size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRhhmMkyv1qU",
        "outputId": "1792d06e-6d8a-4300-d32b-30c2e5c819c3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2 chunks...\n",
            "Processing chunk 1/2\n",
            "Processing chunk 2/2\n",
            "\n",
            "Q&A pairs saved to: qa_pairs_20250117_200649.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "qa_pairs = pd.read_csv(output_file)\n",
        "qa_pairs.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "uikEPzT93-1f",
        "outputId": "09d7f2ba-e93e-4c28-bea1-e3bbbe29855c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   chunk_number                                           question  \\\n",
              "0             1  What should you check first when troubleshooti...   \n",
              "1             1  What should you inspect for potential issues r...   \n",
              "2             1  What is a suggested test to perform when tryin...   \n",
              "3             2  What is one possible reason why water might no...   \n",
              "4             2  How long should you unplug the microwave befor...   \n",
              "5             2  Besides unplugging the microwave, what other s...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Ensure the microwave is plugged into a functio...   \n",
              "1  Make sure the microwave door is fully closed a...   \n",
              "2  Place a cup of water inside and run the microw...   \n",
              "3  The issue may be with the magnetron or other i...   \n",
              "4                                    For 30 seconds.   \n",
              "5  Contact Samsung Support with the model and ser...   \n",
              "\n",
              "                                             context  \n",
              "0  Title: Fixing Heating Issues in Samsung Microw...  \n",
              "1  Title: Fixing Heating Issues in Samsung Microw...  \n",
              "2  Title: Fixing Heating Issues in Samsung Microw...  \n",
              "3  wer for 1-2 minutes. If the water doesn’t heat...  \n",
              "4  wer for 1-2 minutes. If the water doesn’t heat...  \n",
              "5  wer for 1-2 minutes. If the water doesn’t heat...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8825459d-7637-415d-aafd-bd9475397df3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk_number</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>What should you check first when troubleshooti...</td>\n",
              "      <td>Ensure the microwave is plugged into a functio...</td>\n",
              "      <td>Title: Fixing Heating Issues in Samsung Microw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>What should you inspect for potential issues r...</td>\n",
              "      <td>Make sure the microwave door is fully closed a...</td>\n",
              "      <td>Title: Fixing Heating Issues in Samsung Microw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>What is a suggested test to perform when tryin...</td>\n",
              "      <td>Place a cup of water inside and run the microw...</td>\n",
              "      <td>Title: Fixing Heating Issues in Samsung Microw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>What is one possible reason why water might no...</td>\n",
              "      <td>The issue may be with the magnetron or other i...</td>\n",
              "      <td>wer for 1-2 minutes. If the water doesn’t heat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>How long should you unplug the microwave befor...</td>\n",
              "      <td>For 30 seconds.</td>\n",
              "      <td>wer for 1-2 minutes. If the water doesn’t heat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>Besides unplugging the microwave, what other s...</td>\n",
              "      <td>Contact Samsung Support with the model and ser...</td>\n",
              "      <td>wer for 1-2 minutes. If the water doesn’t heat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8825459d-7637-415d-aafd-bd9475397df3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8825459d-7637-415d-aafd-bd9475397df3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8825459d-7637-415d-aafd-bd9475397df3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cc3aa4f0-6c2b-48c3-8e0a-d21caae96e0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc3aa4f0-6c2b-48c3-8e0a-d21caae96e0c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cc3aa4f0-6c2b-48c3-8e0a-d21caae96e0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "qa_pairs",
              "summary": "{\n  \"name\": \"qa_pairs\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"chunk_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"What should you check first when troubleshooting a Samsung microwave that isn't heating properly?\",\n          \"What should you inspect for potential issues related to the door of a Samsung microwave oven?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Ensure the microwave is plugged into a functional power outlet and there is no visible damage to the power cord.\",\n          \"Make sure the microwave door is fully closed and that the seals are clean and intact.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"wer for 1-2 minutes. If the water doesn\\u2019t heat, the issue may be with the magnetron or other internal components.\\nReset Microwave:\\nUnplug the unit for 30 seconds, then plug it back in and restart.\\nContact Samsung Service:\\nIf none of these steps work, contact Samsung Support with the model and serial number of your microwave for further assistance.\",\n          \"Title: Fixing Heating Issues in Samsung Microwave Ovens\\nSamsung microwaves are designed for efficient cooking and reheating. If the microwave is not heating properly, follow these steps:\\nInspect Power Supply:\\nEnsure the microwave is properly plugged into a functional power outlet.\\nCheck for visible damage to the power cord.\\nCheck Door Seal:\\nEnsure the microwave door is fully closed and the seals are clean and intact.\\nTest Cooking Mode:\\nPlace a cup of water inside and run the microwave on high po\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}